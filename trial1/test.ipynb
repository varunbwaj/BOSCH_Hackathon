{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First is cell experimental, pip install camelot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install dependencies\n",
    "!pip install PyMuPDF, \"camelot-py[cv]\","
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import fitz  # PyMuPDF\n",
    "# import pandas as pd\n",
    "# from PIL import Image\n",
    "# import io\n",
    "# import os\n",
    "\n",
    "# def extract_text_from_pdf(pdf_path):\n",
    "#     document = fitz.open(pdf_path)\n",
    "#     text = \"\"\n",
    "#     for page_num in range(len(document)):\n",
    "#         page = document.load_page(page_num)\n",
    "#         text += page.get_text()\n",
    "#     return text\n",
    "\n",
    "# def extract_tables_from_pdf(pdf_path):\n",
    "#     # Note: PyMuPDF does not directly support table extraction.\n",
    "#     #       You would need to integrate with a library like Camelot for more accurate table extraction.\n",
    "#     import camelot\n",
    "\n",
    "#     tables = camelot.read_pdf(pdf_path, pages='all')\n",
    "#     tables_list = [table.df for table in tables]\n",
    "#     return tables_list\n",
    "\n",
    "# def extract_images_from_pdf(pdf_path, output_folder):\n",
    "#     document = fitz.open(pdf_path)\n",
    "#     image_list = []\n",
    "\n",
    "#     for page_num in range(len(document)):\n",
    "#         page = document.load_page(page_num)\n",
    "#         images = page.get_images(full=True)\n",
    "        \n",
    "#         for img_index, img in enumerate(images):\n",
    "#             xref = img[0]\n",
    "#             base_image = document.extract_image(xref)\n",
    "#             image_bytes = base_image[\"image\"]\n",
    "#             image_ext = base_image[\"ext\"]\n",
    "#             image = Image.open(io.BytesIO(image_bytes))\n",
    "\n",
    "#             image_filename = f\"page_{page_num + 1}_img_{img_index + 1}.{image_ext}\"\n",
    "#             image_path = os.path.join(output_folder, image_filename)\n",
    "#             image.save(image_path)\n",
    "#             image_list.append(image_path)\n",
    "\n",
    "#     return image_list\n",
    "\n",
    "# # Usage example:\n",
    "# pdf_path = 'path/to/your/document.pdf'\n",
    "# output_folder = 'path/to/save/images'\n",
    "\n",
    "# # Extract text\n",
    "# text = extract_text_from_pdf(pdf_path)\n",
    "# print(\"Extracted Text:\\n\", text)\n",
    "\n",
    "# # Extract tables\n",
    "# tables = extract_tables_from_pdf(pdf_path)\n",
    "# for i, table in enumerate(tables):\n",
    "#     print(f\"Table {i + 1}:\\n\", table)\n",
    "\n",
    "# # Extract images\n",
    "# if not os.path.exists(output_folder):\n",
    "#     os.makedirs(output_folder)\n",
    "# images = extract_images_from_pdf(pdf_path, output_folder)\n",
    "# print(\"Extracted Images:\", images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON saved to exter.json\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "from PIL import Image\n",
    "import io\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForTokenClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "# Initialize the tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('dbmdz/bert-large-cased-finetuned-conll03-english')\n",
    "model = BertForTokenClassification.from_pretrained('dbmdz/bert-large-cased-finetuned-conll03-english')\n",
    "nlp = pipeline('ner', model=model, tokenizer=tokenizer)\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    document = fitz.open(pdf_path)\n",
    "    text_data = []\n",
    "    for page_num in range(len(document)):\n",
    "        page = document.load_page(page_num)\n",
    "        text = page.get_text(\"text\")\n",
    "        text_data.append({\"page\": page_num + 1, \"text\": text})\n",
    "    return text_data\n",
    "\n",
    "def extract_images_from_pdf(pdf_path, output_folder):\n",
    "    document = fitz.open(pdf_path)\n",
    "    image_list = []\n",
    "\n",
    "    for page_num in range(len(document)):\n",
    "        page = document.load_page(page_num)\n",
    "        images = page.get_images(full=True)\n",
    "        \n",
    "        for img_index, img in enumerate(images):\n",
    "            xref = img[0]\n",
    "            base_image = document.extract_image(xref)\n",
    "            image_bytes = base_image[\"image\"]\n",
    "            image_ext = base_image[\"ext\"]\n",
    "            image = Image.open(io.BytesIO(image_bytes))\n",
    "\n",
    "            image_filename = f\"page_{page_num + 1}_img_{img_index + 1}.{image_ext}\"\n",
    "            image_path = os.path.join(output_folder, image_filename)\n",
    "            image.save(image_path)\n",
    "            image_list.append({\"page\": page_num + 1, \"image_path\": image_path})\n",
    "\n",
    "    return image_list\n",
    "\n",
    "def extract_sections_and_headings(text):\n",
    "    # Use the NLP model to extract named entities which can be used as headings/sections\n",
    "    nlp_results = nlp(text)\n",
    "    sections = []\n",
    "    current_section = {\"title\": \"Introduction\", \"content\": \"\"}\n",
    "    \n",
    "    for result in nlp_results:\n",
    "        if result['entity'] == 'B-MISC':  # Assuming sections/headings are categorized as MISC\n",
    "            if current_section[\"content\"]:\n",
    "                sections.append(current_section)\n",
    "            current_section = {\"title\": result['word'], \"content\": \"\"}\n",
    "        current_section[\"content\"] += \" \" + text[result['start']:result['end']]\n",
    "    \n",
    "    if current_section[\"content\"]:\n",
    "        sections.append(current_section)\n",
    "    \n",
    "    return sections\n",
    "\n",
    "def create_json_structure(text_data, image_data):\n",
    "    manual_data = {\n",
    "        \"title\": \"Car Owner's Manual\",\n",
    "        \"author\": \"Manufacturer Name\",\n",
    "        \"content\": []\n",
    "    }\n",
    "\n",
    "    for text in text_data:\n",
    "        page_num = text[\"page\"]\n",
    "        images_on_page = [img for img in image_data if img[\"page\"] == page_num]\n",
    "        \n",
    "        sections = extract_sections_and_headings(text[\"text\"])\n",
    "        \n",
    "        page_data = {\n",
    "            \"page\": page_num,\n",
    "            \"sections\": sections,\n",
    "            \"images\": images_on_page\n",
    "        }\n",
    "        manual_data[\"content\"].append(page_data)\n",
    "\n",
    "    return manual_data\n",
    "\n",
    "# Usage example:\n",
    "pdf_path = '/Users/varunbharadwaj/Desktop/BOSCH_Hackathon/manuals/exter.pdf'\n",
    "output_folder = 'images_exter'\n",
    "\n",
    "# Extract text\n",
    "text_data = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "# Extract images\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "image_data = extract_images_from_pdf(pdf_path, output_folder)\n",
    "\n",
    "# Create JSON structure\n",
    "manual_json = create_json_structure(text_data, image_data)\n",
    "\n",
    "# Save to JSON file\n",
    "json_path = 'exter.json'\n",
    "with open(json_path, 'w') as json_file:\n",
    "    json.dump(manual_json, json_file, indent=4)\n",
    "\n",
    "print(f\"JSON saved to {json_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'file_size': 2309819, 'creation_date': '2024-05-18T20:19:09.220644', 'modification_date': '2024-05-18T20:19:09.220644', 'custom_metadata': {'title': \"Car Owner's Manual\", 'author': 'TATA Nexon', 'creation_date': 'N/A'}}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "def get_json_metadata(json_path):\n",
    "    # Get file size\n",
    "    file_size = os.path.getsize(json_path)\n",
    "\n",
    "    # Get creation and modification date\n",
    "    creation_time = os.path.getctime(json_path)\n",
    "    modification_time = os.path.getmtime(json_path)\n",
    "\n",
    "    # Convert to human-readable format\n",
    "    creation_date = datetime.fromtimestamp(creation_time).isoformat()\n",
    "    modification_date = datetime.fromtimestamp(modification_time).isoformat()\n",
    "\n",
    "    # Read the JSON file to extract custom metadata\n",
    "    with open(json_path, 'r') as json_file:\n",
    "        data = json.load(json_file)\n",
    "\n",
    "    # Example custom metadata extraction (adjust according to your JSON structure)\n",
    "    metadata = {\n",
    "        \"title\": data.get(\"title\", \"N/A\"),\n",
    "        \"author\": data.get(\"author\", \"N/A\"),\n",
    "        \"creation_date\": data.get(\"creation_date\", \"N/A\")\n",
    "    }\n",
    "\n",
    "    # Combine all metadata\n",
    "    file_metadata = {\n",
    "        \"file_size\": file_size,\n",
    "        \"creation_date\": creation_date,\n",
    "        \"modification_date\": modification_date,\n",
    "        \"custom_metadata\": metadata\n",
    "    }\n",
    "\n",
    "    return file_metadata\n",
    "\n",
    "# Usage example\n",
    "json_path = 'exter.json'\n",
    "metadata = get_json_metadata(json_path)\n",
    "print(metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    \"title\": \"Car Owner's Manual\",\n",
    "    \"author\": \"TATA Nexon\",\n",
    "    \"content\": [\n",
    "        {\n",
    "            \"page\": 12,\n",
    "            \"sections\": [\n",
    "                {\n",
    "                    \"title\": \"Introduction\",\n",
    "                    \"content\": \" turning \\nDriving On Gradients \\nWhen climbing gradient, the vehicle may \\nbegin to slow down and show a lack of \\npower. If this happens, shift to a lower gear \\nand apply power smoothly so that there is \\nno loss of traction. \\nWhen driving down a hill, the engine brak\\u00ad\\ning should be used by shifting into a lower \\ngear. Do not drive in neutral gear or switch \\noff the engine.\\n \\n \\nDriving On Highway  \\nStopping distance progressively, in-\\ncreases with vehicle speed. Maintain a \\nsufficient distance between your vehicle \\nand the vehicle ahead.  \\nFor long distance driving, perform safety \\nchecks before starting a trip and take rest \\nat certain intervals to prevent fatigue\\nSEAT BELTS \\nThis section of user manual describes \\nyour Vehicle\\u2019s seat belt, airbag and Child \\nrestraints system. Please read and follow \\nall these instructions care-fully to minimise \\nrisk of severe injury or death. \\nSeat belts are the primary restraints \\n\\u2022\\nsystem in the vehicle. All occupants, \\nincluding the driver, should always \\nwear their seat belts to minimize the \\nrisk of injury.  \\nSit back and adjust (if equipped), the \\n\\u2022\\nseat. Make sure that your seat is ad\\u00ad\\njusted to a good driving position and \\nthe back of the seat is upright. \\nBuckling The Shoulder Seat Belt \\nGrasp the tongue then slowly pull out \\n\\u2022\\nthe seat belt over the shoulder and \\nacross the chest. When the seat belt is \\nlong enough to fit, insert the tongue \\ninto the lock buckle until you hear a \\n\\u201cCLICK\\u201d which indicates that the seat \\nbelt is securely locked. \\nPosition the lap portion of seat belt \\n\\u2022\\nacross your pelvic bone , below your \\n WARNING\\nOn long and steep gradients you must \\nreduce the load on the brakes by shift\\u00ad\\ning early to a lower gear. This allows \\nyou to take ad-vantage of the engine \\nbraking effect and helps avoid over\\u00ad\\nheating of service brakes resulting in re\\u00ad\\nduced braking efficiency\\nSAFETY\\n3\\n\"\n",
    "                }\n",
    "            ],\n",
    "            \"images\": [\n",
    "                {\n",
    "                    \"page\": 12,\n",
    "                    \"image_path\": \"images/page_12_img_1.png\"\n",
    "                },\n",
    "                {\n",
    "                    \"page\": 12,\n",
    "                    \"image_path\": \"images/page_12_img_2.png\"\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index and metadata saved.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Initialize the sentence transformer model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Function to process a single JSON file\n",
    "def process_json_file(json_path):\n",
    "    with open(json_path, 'r') as json_file:\n",
    "        data = json.load(json_file)\n",
    "\n",
    "    texts = []\n",
    "    metadata = []\n",
    "\n",
    "    for page in data['content']:\n",
    "        page_num = page['page']\n",
    "        for section in page['sections']:\n",
    "            section_title = section['title']\n",
    "            section_content = section['content']\n",
    "            text = f\"Page {page_num}, Section: {section_title}, Content: {section_content}\"\n",
    "            texts.append(text)\n",
    "            metadata.append({\n",
    "                \"file\": os.path.basename(json_path),\n",
    "                \"page\": page_num,\n",
    "                \"section_title\": section_title\n",
    "            })\n",
    "\n",
    "    return texts, metadata\n",
    "\n",
    "# Directory containing JSON files\n",
    "json_dir = '/Users/varunbharadwaj/Desktop/BOSCH_Hackathon/trial1/json_files'\n",
    "\n",
    "# Initialize lists to hold all texts and metadata\n",
    "all_texts = []\n",
    "all_metadata = []\n",
    "\n",
    "# Process each JSON file in the directory\n",
    "for filename in os.listdir(json_dir):\n",
    "    if filename.endswith('.json'):\n",
    "        json_path = os.path.join(json_dir, filename)\n",
    "        texts, metadata = process_json_file(json_path)\n",
    "        all_texts.extend(texts)\n",
    "        all_metadata.extend(metadata)\n",
    "\n",
    "# Convert texts to embeddings\n",
    "embeddings = model.encode(all_texts, convert_to_tensor=False)\n",
    "\n",
    "# Convert embeddings to a numpy array\n",
    "embeddings = np.array(embeddings)\n",
    "\n",
    "# Create a FAISS index\n",
    "d = embeddings.shape[1]  # Dimension of embeddings\n",
    "index = faiss.IndexFlatL2(d)\n",
    "\n",
    "# Add embeddings to the index\n",
    "index.add(embeddings)\n",
    "\n",
    "# Save the FAISS index and metadata\n",
    "faiss.write_index(index, 'manual_index.faiss')\n",
    "\n",
    "with open('manual_metadata.json', 'w') as meta_file:\n",
    "    json.dump(all_metadata, meta_file)\n",
    "\n",
    "print(\"FAISS index and metadata saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 results:\n",
      "{'file': 'nexon.json', 'page': 12, 'section_title': 'Introduction'}\n",
      "{'file': 'nexon.json', 'page': 145, 'section_title': 'Introduction'}\n",
      "{'file': 'exter.json', 'page': 237, 'section_title': 'Introduction'}\n",
      "{'file': 'exter.json', 'page': 259, 'section_title': 'Introduction'}\n",
      "{'file': 'nexon.json', 'page': 124, 'section_title': 'Introduction'}\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import json\n",
    "\n",
    "# Load the FAISS index and metadata\n",
    "index = faiss.read_index('manual_index.faiss')\n",
    "\n",
    "with open('manual_metadata.json', 'r') as meta_file:\n",
    "    metadata = json.load(meta_file)\n",
    "\n",
    "# Initialize the sentence transformer model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Function to query the vector database\n",
    "def query_manual(query_text, top_k=5):\n",
    "    # Vectorize the query\n",
    "    query_embedding = model.encode([query_text])\n",
    "    \n",
    "    # Search the index\n",
    "    distances, indices = index.search(query_embedding, top_k)\n",
    "    \n",
    "    # Retrieve and print the results\n",
    "    results = []\n",
    "    for idx in indices[0]:\n",
    "        result = metadata[idx]\n",
    "        results.append(result)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example query\n",
    "query = \"How to drive on gradients?\"\n",
    "results = query_manual(query)\n",
    "print(\"Top 5 results:\")\n",
    "for res in results:\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index and metadata saved.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Directory containing JSON files\n",
    "json_dir = 'json_files'\n",
    "\n",
    "# Initialize the sentence transformer model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Initialize lists to hold the text and metadata\n",
    "texts = []\n",
    "metadata = []\n",
    "\n",
    "# Loop through each JSON file in the directory\n",
    "for json_file_name in os.listdir(json_dir):\n",
    "    if json_file_name.endswith('.json'):\n",
    "        json_path = os.path.join(json_dir, json_file_name)\n",
    "        \n",
    "        # Load JSON data\n",
    "        with open(json_path, 'r') as json_file:\n",
    "            data = json.load(json_file)\n",
    "        \n",
    "        # Extract text and metadata from the JSON structure\n",
    "        for page in data['content']:\n",
    "            page_num = page['page']\n",
    "            for section in page['sections']:\n",
    "                section_title = section['title']\n",
    "                section_content = section['content']\n",
    "                text = f\"Page {page_num}, Section: {section_title}, Content: {section_content}\"\n",
    "                texts.append(text)\n",
    "                metadata.append({\n",
    "                    \"file_name\": json_file_name,\n",
    "                    \"page\": page_num,\n",
    "                    \"section_title\": section_title,\n",
    "                    \"section_content\": section_content\n",
    "                })\n",
    "\n",
    "# Convert texts to embeddings\n",
    "embeddings = model.encode(texts, convert_to_tensor=False)\n",
    "\n",
    "# Convert embeddings to a numpy array\n",
    "embeddings = np.array(embeddings)\n",
    "\n",
    "# Create a FAISS index\n",
    "d = embeddings.shape[1]  # Dimension of embeddings\n",
    "index = faiss.IndexFlatL2(d)\n",
    "\n",
    "# Add embeddings to the index\n",
    "index.add(embeddings)\n",
    "\n",
    "# Save the FAISS index and metadata\n",
    "faiss.write_index(index, 'manuals_index.faiss')\n",
    "\n",
    "with open('manuals_metadata.json', 'w') as meta_file:\n",
    "    json.dump(metadata, meta_file)\n",
    "\n",
    "print(\"FAISS index and metadata saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 results:\n",
      "File: nexon.json, Page: 12, Section Title: Introduction\n",
      "Content:  turning \n",
      "Driving On Gradients \n",
      "When climbing gradient, the vehicle may \n",
      "begin to slow down and show a lack of \n",
      "power. If this happens, shift to a lower gear \n",
      "and apply power smoothly so that there is \n",
      "no loss of traction. \n",
      "When driving down a hill, the engine brak­\n",
      "ing should be used by shifting into a lower \n",
      "gear. Do not drive in neutral gear or switch \n",
      "off the engine.\n",
      " \n",
      " \n",
      "Driving On Highway  \n",
      "Stopping distance progressively, in-\n",
      "creases with vehicle speed. Maintain a \n",
      "sufficient distance between your vehicle \n",
      "and the vehicle ahead.  \n",
      "For long distance driving, perform safety \n",
      "checks before starting a trip and take rest \n",
      "at certain intervals to prevent fatigue\n",
      "SEAT BELTS \n",
      "This section of user manual describes \n",
      "your Vehicle’s seat belt, airbag and Child \n",
      "restraints system. Please read and follow \n",
      "all these instructions care-fully to minimise \n",
      "risk of severe injury or death. \n",
      "Seat belts are the primary restraints \n",
      "•\n",
      "system in the vehicle. All occupants, \n",
      "including the driver, should always \n",
      "wear their seat belts to minimize the \n",
      "risk of injury.  \n",
      "Sit back and adjust (if equipped), the \n",
      "•\n",
      "seat. Make sure that your seat is ad­\n",
      "justed to a good driving position and \n",
      "the back of the seat is upright. \n",
      "Buckling The Shoulder Seat Belt \n",
      "Grasp the tongue then slowly pull out \n",
      "•\n",
      "the seat belt over the shoulder and \n",
      "across the chest. When the seat belt is \n",
      "long enough to fit, insert the tongue \n",
      "into the lock buckle until you hear a \n",
      "“CLICK” which indicates that the seat \n",
      "belt is securely locked. \n",
      "Position the lap portion of seat belt \n",
      "•\n",
      "across your pelvic bone , below your \n",
      " WARNING\n",
      "On long and steep gradients you must \n",
      "reduce the load on the brakes by shift­\n",
      "ing early to a lower gear. This allows \n",
      "you to take ad-vantage of the engine \n",
      "braking effect and helps avoid over­\n",
      "heating of service brakes resulting in re­\n",
      "duced braking efficiency\n",
      "SAFETY\n",
      "3\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import json\n",
    "\n",
    "# Load the FAISS index and metadata\n",
    "index = faiss.read_index('manuals_index.faiss')\n",
    "\n",
    "with open('manuals_metadata.json', 'r') as meta_file:\n",
    "    metadata = json.load(meta_file)\n",
    "\n",
    "# Initialize the sentence transformer model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Function to query the vector database\n",
    "def query_manual(query_text, top_k=5):\n",
    "    # Vectorize the query\n",
    "    query_embedding = model.encode([query_text])\n",
    "    \n",
    "    # Search the index\n",
    "    distances, indices = index.search(query_embedding, top_k)\n",
    "    \n",
    "    # Retrieve and print the results\n",
    "    results = []\n",
    "    for idx in indices[0]:\n",
    "        result = metadata[idx]\n",
    "        results.append(result)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example query\n",
    "query = \"How to drive on gradients?\"\n",
    "results = query_manual(query,1)\n",
    "for res in results:\n",
    "    print(f\"File: {res['file_name']}, Page: {res['page']}, Section Title: {res['section_title']}\")\n",
    "    print(f\"Content: {res['section_content']}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: exter.json, Page: 302, Section Title: Introduction\n",
      "Content:  9-22\n",
      "Maintenance\n",
      "ENGINE OIL\n",
      "Engine oil is used for lubricating, \n",
      "cooling, and operating various hydraulic \n",
      "components in the engine. Engine oil \n",
      "consumption while driving is normal, \n",
      "and it is necessary to check and refill the \n",
      "engine oil regularly. Also, check and refill \n",
      "the oil level within the recommended \n",
      "maintenance schedule to prevent \n",
      "deterioration of oil performance.\n",
      "Check the engine oil following the below \n",
      "procedure.\n",
      "Checking the engine oil level\n",
      "1. Follow all of the oil manufacturer’s \n",
      "precautions.\n",
      "2. Be sure the vehicle is on the lovel \n",
      "ground in P (Park) with the parking \n",
      "brake set. if possible, block the \n",
      "wheels.\n",
      "3. Turn the engine on and warm \n",
      "the engine up until the coolant \n",
      "temperature reaches a constant \n",
      "normal temperature.\n",
      "4. Turn the engine off, remove the oil \n",
      "filler cap and pull the dipstick out. \n",
      "Wait for 15 minutes for the oil to return \n",
      "to the oil pan.\n",
      "5. Wipe the dipstick clean and re-insert it \n",
      "fully.\n",
      "OAI3089002\n",
      "OAI3089002\n",
      "6. Pull the dipstick out again and check \n",
      "the level.\n",
      "OAI3089003\n",
      "OAI3089003\n",
      "7. If the oil level is below L, add enough \n",
      "oil to bring the level to F (Full). Do not \n",
      "overfill.\n",
      "Use a funnel to help prevent oil from \n",
      "being spilled on engine components.\n",
      "Use only the specified engine oil. (Refer \n",
      "to the “Recommended lubricants and \n",
      "capacities” in chapter 2.)\n",
      " WARNING\n",
      "Radiator hoseBe very careful not to \n",
      "touch the radiator hose when checking \n",
      "or adding the engine oil as it may be \n",
      "hot enough to burn you. \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"Oil change\"\n",
    "results = query_manual(query,1)\n",
    "for res in results:\n",
    "    print(f\"File: {res['file_name']}, Page: {res['page']}, Section Title: {res['section_title']}\")\n",
    "    print(f\"Content: {res['section_content']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index and metadata saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "546616d69bf946f085d8b18fffd3c66f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/473 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42d51232b6d94778b9545fb2ff995b2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/261M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "314cd2dbfc904fbcab7a265a73967811",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e33a44847d84b3ba7c7858e4bbe8a89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0a04fc405c94546b27ec5552fe08303",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Answer:\n",
      "{'score': 0.36999034881591797, 'start': 1670, 'end': 1702, 'answer': 'shift\\xad\\ning early to a lower gear'}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "from transformers import pipeline\n",
    "\n",
    "# Directory containing JSON files\n",
    "json_dir = 'json_files'\n",
    "\n",
    "# Initialize the sentence transformer model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Initialize lists to hold the text and metadata\n",
    "texts = []\n",
    "metadata = []\n",
    "\n",
    "# Loop through each JSON file in the directory\n",
    "for json_file_name in os.listdir(json_dir):\n",
    "    if json_file_name.endswith('.json'):\n",
    "        json_path = os.path.join(json_dir, json_file_name)\n",
    "        \n",
    "        # Load JSON data\n",
    "        with open(json_path, 'r') as json_file:\n",
    "            data = json.load(json_file)\n",
    "        \n",
    "        # Extract text and metadata from the JSON structure\n",
    "        for page in data['content']:\n",
    "            page_num = page['page']\n",
    "            for section in page['sections']:\n",
    "                section_title = section['title']\n",
    "                section_content = section['content']\n",
    "                text = f\"Page {page_num}, Section: {section_title}, Content: {section_content}\"\n",
    "                texts.append(text)\n",
    "                metadata.append({\n",
    "                    \"file_name\": json_file_name,\n",
    "                    \"page\": page_num,\n",
    "                    \"section_title\": section_title,\n",
    "                    \"section_content\": section_content\n",
    "                })\n",
    "\n",
    "# Convert texts to embeddings\n",
    "embeddings = model.encode(texts, convert_to_tensor=False)\n",
    "\n",
    "# Convert embeddings to a numpy array\n",
    "embeddings = np.array(embeddings)\n",
    "\n",
    "# Create a FAISS index\n",
    "d = embeddings.shape[1]  # Dimension of embeddings\n",
    "index = faiss.IndexFlatL2(d)\n",
    "\n",
    "# Add embeddings to the index\n",
    "index.add(embeddings)\n",
    "\n",
    "# Save the FAISS index and metadata\n",
    "faiss.write_index(index, 'manuals_index.faiss')\n",
    "\n",
    "with open('manuals_metadata.json', 'w') as meta_file:\n",
    "    json.dump(metadata, meta_file)\n",
    "\n",
    "print(\"FAISS index and metadata saved.\")\n",
    "\n",
    "# Load the FAISS index and metadata\n",
    "index = faiss.read_index('manuals_index.faiss')\n",
    "\n",
    "with open('manuals_metadata.json', 'r') as meta_file:\n",
    "    metadata = json.load(meta_file)\n",
    "\n",
    "# Initialize the sentence transformer model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Initialize the QA pipeline\n",
    "qa_pipeline = pipeline(\"question-answering\")\n",
    "\n",
    "# Function to query the vector database\n",
    "def query_manual(query_text, top_k=5):\n",
    "    # Vectorize the query\n",
    "    query_embedding = model.encode([query_text])\n",
    "    \n",
    "    # Search the index\n",
    "    distances, indices = index.search(query_embedding, top_k)\n",
    "    \n",
    "    # Retrieve the results\n",
    "    results = []\n",
    "    for idx in indices[0]:\n",
    "        result = metadata[idx]\n",
    "        results.append(result)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Function to generate answers\n",
    "def generate_answer(query_text, top_k=5):\n",
    "    results = query_manual(query_text, top_k)\n",
    "    \n",
    "    # Prepare context from retrieved sections\n",
    "    context = \" \".join([res['section_content'] for res in results])\n",
    "    \n",
    "    # Use the QA model to generate the answer\n",
    "    answer = qa_pipeline(question=query_text, context=context)\n",
    "    \n",
    "    return answer\n",
    "\n",
    "# Example query\n",
    "query = \"How to drive on gradients?\"\n",
    "answer = generate_answer(query)\n",
    "print(\"Generated Answer:\")\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bosch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
